<html>
  <head>
    <title> Frederic Koehler </title>
	<style type="text/css">
    		body{
    		    line-height: 1.5;
    		    padding: 4em 1em;
    		    width:800px;
    		    margin-left:auto;
    		    margin-right:auto;
		    font-size:1.05em;
    		}
		li{
 	   	    margin: .5em 0;
		}
	</style>
  </head>
  <body>
    <img src="headshot.jpg" style="float:right;height:250px" hspace="20px">
    <h1> Frederic Koehler </h1>
    <p> Hi! I am currently a Simons-Berkeley Research Fellow at UC Berkeley's <a href="https://simons.berkeley.edu/">Simons Institute</a> for this Fall, and I will be a Motwani Postdoctoral Fellow at Stanford afterwords. I recently received my PHD in <em>Mathematics and Statistics</em> from MIT, where I was coadvised by
    <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a> and <a href="http://math.mit.edu/~elmos/">Elchanan Mossel</a>. Before that, I received my undergraduate degree in Mathematics at Princeton University. My current research interests include computational learning theory and related topics: probability theory, high-dimensional statistics, optimization, related aspects of statistical physics, etc. In particular, I am very interested in learning and inference in graphical models. </p>
    <p> To reach me: [first initial][last name]@berkeley.edu  </p>
    <h2>Publications and Preprints </h2>
    <ol reversed>
	    <li> <em>Uniform Convergence of Interpolators: Gaussian Width, Norm Bounds, and Benign Overfitting</em>, joint with <a href="https://stat.uchicago.edu/people/profile/lijia-zhou/">Lijia Zhou</a>, <a href="https://djsutherland.ml">Danica J. Sutherland</a>, and <a href="https://home.ttic.edu/~nati/">Nathan Srebro</a>. <a href="https://arxiv.org/abs/2106.09276">[arXiv:2106.09276]</a></li>
	<li> <em>Entropic Independence in High-Dimensional Expanders: Modified Log-Sobolev Inequalities for Fractionally Log-Concave Polynomials and the Ising Model</em>, joint with <a href="https://nimaanari.com">Nima Anari</a>, <a href="https://jainvishesh.github.io">Vishesh Jain</a>, <a href="https://web.stanford.edu/~huypham/">Huy Tuan Pham</a>, and <a href="https://tdvuong.people.stanford.edu/about">Thuy-Duong Vuong</a>. <a href="https://arxiv.org/abs/2106.04105">[arXiv:2106.04105]</a></li>
	<li> <em>On the Power of Preconditioning in Sparse Linear Regression</em>, joint with <a href="https://math.mit.edu/~kelner/">Jon Kelner</a>, <a href="https://raghumeka.github.io">Raghu Meka</a>, and <a href="http://www.mit.edu/~drohatgi/">Dhruv Rohatgi</a>. Symposium on Foundations of Computer Science (FOCS) 2021, To Appear. <a href="https://arxiv.org/abs/2106.09207">[arXiv:2106.09207]</a> </li>
	<li> <em>Chow-Liu++: Optimal Prediction-Centric Learning of Tree Ising Models</em>, joint with <a href="http://web.mit.edu/eboix/www/">Enric Boix-Adser&agrave;</a> and <a href="https://www.mit.edu/~gbresler/">Guy Bresler</a>. Symposium on Foundations of Computer Science (FOCS) 2021, To Appear. <a href="https://arxiv.org/abs/2106.03969">[arXiv:2106.03969]</a></li>
	<li> <em>Online and Distribution-Free Robustness: Regression and Contextual Bandits with Huber Contamination</em>, joint with  <a href="http://people.csail.mit.edu/sitanc/">Sitan Chen</a>, <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a>, and <a href="https://scholar.google.com/citations?user=maa1m0oAAAAJ&hl=en">Morris Yau</a>. Symposium on Foundations of Computer Science (FOCS) 2021, To Appear.
		<a href="https://arxiv.org/abs/2010.04157">[arXiv:2010.04157]</a>.  </li>
	<li> <em>Multidimensional Scaling: Approximation and Complexity</em>, joint with <a href="http://erikdemaine.org">Erik Demaine</a>, <a href="https://www.seas.harvard.edu/person/adam-hesterberg">Adam Hesterberg</a>, <a href="https://cs.uwaterloo.ca/about/people/jayson-lynch">Jayson Lynch</a>, and <a href="https://math.mit.edu/~urschel/">John Urschel</a>. International Conference on Machine Learning (ICML) 2021. </li>
	<li> <em>Representational aspects of depth and conditioning in normalizing flows</em>, joint with <a href="https://github.com/virajmehta">Viraj Mehta</a> and <a href="https://www.andrew.cmu.edu/user/aristesk/">Andrej Risteski</a>.
		International Conference on Machine Learning (ICML) 2021. <a href="https://arxiv.org/abs/2010.01155">[arXiv:2010.01155]</a>. </li>
	<li> <em>From Boltzmann Machines to Neural Networks and Back Again</em>, joint with <a href="https://www.cs.utexas.edu/~surbhi/">Surbhi Goel</a> and <a href="https://www.cs.utexas.edu/~klivans/">Adam Klivans</a>. Neural Information Processing Systems (NeurIPS) 2020.
		<a href="https://arxiv.org/abs/2007.12815">[arXiv:2007.12815]</a> </li>
	<li> <em>A Spectral Condition for Spectral Gap: Fast Mixing in High-Temperature Ising Models</em>, joint with <a href="http://www.wisdom.weizmann.ac.il/~ronene/">Ronen Eldan</a> and <a href="http://www.wisdom.weizmann.ac.il/~zeitouni/">Ofer Zeitouni</a>. Probability Theory and Related Fields (PTRF) 2021+, To Appear.
		<a href="https://arxiv.org/abs/2007.08200">[arXiv:2007.08200]</a> </li>
	<li> <em>Classification Under Misspecification: Halfspaces, Generalized Linear Models, and Connections to Evolvability</em>, joint with <a href="http://people.csail.mit.edu/sitanc/">Sitan Chen</a>, <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a>, and <a href="https://scholar.google.com/citations?user=maa1m0oAAAAJ&hl=en">Morris Yau</a>. Neural Information Processing Systems (NeurIPS) 2020 (Spotlight Presentation).
		<a href="https://arxiv.org/abs/2006.04787">[arXiv:2006.04787]</a> </li>
	<li> <em>A Phase Transition in Arrow's Theorem</em>, joint with <a href="http://www-math.mit.edu/~elmos/"> Elchanan Mossel</a>. <a href="https://arxiv.org/abs/2004.12580">[arXiv:2004.12580]</a> </li>
	<li> <em>Learning Some Popular Gaussian Graphical Models without Condition Number Bounds</em>, joint with <a href="http://math.mit.edu/~kelner/index.html">Jonathan Kelner</a>, <a href="https://raghumeka.github.io">Raghu Meka</a>, and <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a>. Neural Information Processing Systems (NeurIPS) 2020 (Spotlight Presentation).  <a href="http://arxiv.org/abs/1905.01282">[arXiv:1905.01282]</a>. </li>
	<li> <em>Fast Convergence of Belief Propagation to Global Optima: Beyond Correlation Decay.</em>  Neural Information Processing Systems (NeurIPS) 2019 (Spotlight presentation).
		<a href="https://arxiv.org/abs/1905.09992">[arXiv:1905.09992]</a> </li>
	<li> <em>Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation</em>, joint with <a href="http://math.mit.edu/~visheshj/">Vishesh Jain</a>, <a href="http://web.mit.edu/jingbo/www/">Jingbo Liu</a>, and <a href="http://math.mit.edu/~elmos/">Elchanan Mossel</a>. Conference on Learning Theory (COLT) 2019. <a href="https://arxiv.org/abs/1905.10031"> [arXiv:1905.10031] </a>
	</li>
	<li> <em>How Many Subpopulations is Too Many? Exponential Lower Bounds for Inferring Population Histories</em>, joint with <a href="http://www-math.mit.edu/~younhun/">Younhun Kim</a>, <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a>, 
		<a href="http://www-math.mit.edu/~elmos/"> Elchanan Mossel</a>, and 
		<a href="http://www.mit.edu/~govind/"> Govind Ramnarayan</a>. International Conference on Research in Computational Molecular Biology (RECOMB) 2019; Journal of Computational Biology (JCB) Special Issue. <a href="https://arxiv.org/abs/1811.03177">[arXiv:1811.03177]</a></li>  
	<li> <em>Mean-field approximation, convex hierarchies, and the optimality of correlation rounding: a unified perspective</em>, joint with <a href="http://math.mit.edu/~visheshj/">Vishesh Jain</a> and <a href="https://math.mit.edu/~risteski/">Andrej Risteski</a>. Symposium on Theory of Computing (STOC) 2019. <a href="https://arxiv.org/abs/1808.07226">[arXiv:1808.07226]</a></li>
	<li> <em>Learning Restricted Boltzmann Machines via Influence Maximization</em>, joint with <a href="http://www.mit.edu/~gbresler/">Guy Bresler</a> and <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a>. Symposium on Theory of Computing (STOC) 2019. <a href="https://arxiv.org/abs/1805.10262">[arXiv:1805.10262]</a></li>
	<li> <em> The Comparative Power of ReLU Networks and Polynomial Kernels in the Presence of Sparse Latent Structure</em>, joint with <a href="https://math.mit.edu/~risteski/">Andrej Risteski</a>. International Conference on Learning Representations (ICLR) 2019. <a href="https://arxiv.org/abs/1805.11405">[arXiv:1805.11405]</a> <a href="https://openreview.net/forum?id=rJgTTjA9tX"> [OpenReview] </a> </li>
	<li> <em>The Vertex Sample Complexity of Free Energy is Polynomial</em>, joint with <a href="http://math.mit.edu/~visheshj/">Vishesh Jain</a> and <a href="http://www-math.mit.edu/~elmos/"> Elchanan Mossel</a>. Conference on Learning Theory (COLT) 2018. <a href="https://arxiv.org/abs/1802.06129">[arXiv:1802.06129]</a></li>
	<li> <em>The Mean-Field Approximation: Information Inequalities, Algorithms, and Complexity</em>, joint with <a href="http://math.mit.edu/~visheshj/">Vishesh Jain</a> and <a href="http://www-math.mit.edu/~elmos/"> Elchanan Mossel</a>. Conference on Learning Theory (COLT) 2018. <a href="https://arxiv.org/abs/1802.06126">[arXiv:1802.06126]</a></li>
	<li> <em> Information theoretic properties of Markov random fields, and their algorithmic applications</em>, joint with <a href="https://math.mit.edu/directory/profile.php?pid=1780">Linus Hamilton</a> and <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a>. Neural Information Processing Systems (NeurIPS) 2017. <a href="https://arxiv.org/abs/1705.11107"> [arXiv:1705.11107] </a></li>
	<li> <em> Busy Time Scheduling on a Bounded Number of Machines</em>, joint with <a href="https://www.cs.umd.edu/~samir/">Samir Khuller</a>. Algorithm and Data Structures Symposium (WADS) 2017. (<a href="busytime.pdf">Full Version</a>, <a href="wads_presentation.pdf">slides</a>) </li>
	<li> <em> Provable algorithms for inference in topic models</em>, joint with <a href="https://www.cs.princeton.edu/~arora/">Sanjeev Arora</a>, <a href="https://users.cs.duke.edu/~rongge/">Rong Ge</a>, <a href="https://ai.stanford.edu/~tengyuma/">Tengyu Ma</a>, and <a href="http://people.csail.mit.edu/moitra/">Ankur Moitra</a>. International Conference on Machine Learning (ICML) 2016. <a href="https://arxiv.org/abs/1605.08491"> [arXiv:1605.08491] </a></li>
	<li> <em>Optimal batch schedules for parallel machines</em>, joint with <a href="https://www.cs.umd.edu/~samir/">Samir Khuller</a>. Algorithm and Data Structures Symposium (WADS) 2013.  
<a href="https://www.cs.umd.edu/~samir/grant/sched-wads.pdf">(Full version)</a> </li>
</ol>
    <p> In most cases, authors are listed in alphabetical order, following the convention in mathematics and theoretical computer science. </p>
<!--  <p> (Publications list: for now refer to <a href="https://scholar.google.com/citations?user=Atc5w-4AAAAJ&hl=en"> Google Scholar </a>) </p> 
  
<p> For anyone looking for a full version of Busy Time Scheduling on a Bounded Number of Machines (WADS 2017): (<a href="busytime.pdf"> busytime.pdf </a>, <a href="wads_presentation.pdf"> slides </a>) </p> -->
  <h2> Notes </h2>
  <ol> <li> <a href="tv_note.pdf"> A Note on Minimax Learning of Tree Models </a> </li>
  </ol>
  </body>    
</html>

